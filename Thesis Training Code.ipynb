{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d555d11-30bb-4150-93fa-24ae8dfb952a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/1000 | Recon Loss: 0.1914 | Adv Loss: 0.8340 | D Loss: 0.6188 | Perceptual Loss: 0.7564\n",
      "Epoch 2/1000 | Recon Loss: 0.1719 | Adv Loss: 1.5738 | D Loss: 0.3653 | Perceptual Loss: 0.7164\n",
      "Epoch 3/1000 | Recon Loss: 0.2083 | Adv Loss: 1.8808 | D Loss: 0.2331 | Perceptual Loss: 0.8009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import csv\n",
    "# Disable benchmark mode and enable deterministic mode\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Enable Debugging\n",
    "# ----------------------------\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# ----------------------------\n",
    "# Device Setup\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ----------------------------\n",
    "# Data Preparation\n",
    "# ----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=\"C:/Users/abhis/Downloads/Abisheck_Chandru_Python_code/code/Dataset/healthy\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "# ----------------------------\n",
    "# Define Multi-Scale Perceptual Loss Network with Residual Connections\n",
    "# ----------------------------\n",
    "class MultiScalePerceptualLossNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScalePerceptualLossNetwork, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels, kernel_size, stride, padding):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        \n",
    "        # Two parallel branches with different kernel sizes\n",
    "        self.branch1 = nn.Sequential(\n",
    "            conv_block(3, 64, 3, 1, 1),\n",
    "            conv_block(64, 128, 3, 1, 1),\n",
    "            conv_block(128, 256, 3, 1, 1)\n",
    "        )\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "            conv_block(3, 64, 5, 1, 2),\n",
    "            conv_block(64, 128, 5, 1, 2),\n",
    "            conv_block(128, 256, 5, 1, 2)\n",
    "        )\n",
    "        \n",
    "        # Residual connection to directly map input to feature space\n",
    "        self.residual = nn.Conv2d(3, 256, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.branch1(x) + self.branch2(x) + self.residual(x)\n",
    "\n",
    "perceptual_net = MultiScalePerceptualLossNetwork().to(device)\n",
    "\n",
    "def perceptual_loss(fake, real):\n",
    "    real_feats = perceptual_net(real)\n",
    "    fake_feats = perceptual_net(fake)\n",
    "    return torch.nn.functional.mse_loss(fake_feats, real_feats)\n",
    "\n",
    "# ----------------------------\n",
    "# Define the Vector Quantizer (EMA based)\n",
    "# ----------------------------\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay=0.99, epsilon=1e-5):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embedding.weight.data.uniform_(-1 / num_embeddings, 1 / num_embeddings)\n",
    "\n",
    "        self.ema_cluster_size = torch.zeros(num_embeddings, device=device)\n",
    "        self.ema_weights = torch.zeros_like(self.embedding.weight, device=device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        input_shape = inputs.shape\n",
    "        flat_input = inputs.permute(0, 2, 3, 1).contiguous().view(-1, self.embedding_dim)\n",
    "\n",
    "        distances = (\n",
    "            torch.sum(flat_input ** 2, dim=1, keepdim=True)\n",
    "            + torch.sum(self.embedding.weight ** 2, dim=1)\n",
    "            - 2 * torch.matmul(flat_input, self.embedding.weight.t())\n",
    "        )\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        quantized = torch.matmul(encodings, self.embedding.weight)\n",
    "\n",
    "        quantized = quantized.view(input_shape[0], input_shape[2], input_shape[3], self.embedding_dim)\n",
    "        quantized = quantized.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "        # EMA Update\n",
    "        with torch.no_grad():\n",
    "            self.ema_cluster_size = self.decay * self.ema_cluster_size + (1 - self.decay) * encodings.sum(0)\n",
    "            self.ema_weights = self.decay * self.ema_weights + (1 - self.decay) * torch.matmul(encodings.t(), flat_input)\n",
    "            n = self.ema_cluster_size.sum()\n",
    "            self.ema_cluster_size = ((self.ema_cluster_size + self.epsilon) / (n + self.num_embeddings * self.epsilon) * n)\n",
    "            self.embedding.weight.data = self.ema_weights / self.ema_cluster_size.unsqueeze(1)\n",
    "\n",
    "        e_latent_loss = torch.mean((quantized.detach() - inputs) ** 2)\n",
    "        q_latent_loss = torch.mean((quantized - inputs.detach()) ** 2)\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        return quantized.clone(), loss\n",
    "\n",
    "# ----------------------------\n",
    "# Define Residual Block for Encoder/Decoder\n",
    "# ----------------------------\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Define the Encoder with Residual Connections\n",
    "# ----------------------------\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_dim=256, latent_dim=256):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.Conv2d(hidden_dim, latent_dim, kernel_size=4, stride=2, padding=1),\n",
    "            ResidualBlock(latent_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Define the Decoder with Residual Connections\n",
    "# ----------------------------\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, out_channels=3, latent_dim=256, hidden_dim=256):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            ResidualBlock(latent_dim),\n",
    "            nn.ConvTranspose2d(latent_dim, hidden_dim, kernel_size=4, stride=2, padding=1),\n",
    "            ResidualBlock(hidden_dim),\n",
    "            nn.ConvTranspose2d(hidden_dim, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Define the VQGAN Model\n",
    "# ----------------------------\n",
    "class VQGAN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VQGAN, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.quantizer = VectorQuantizer(1024, 256, 0.25)\n",
    "        self.decoder = Decoder()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        quantized, vq_loss = self.quantizer(z)\n",
    "        x_recon = self.decoder(quantized)\n",
    "        return x_recon, vq_loss\n",
    "\n",
    "# ----------------------------\n",
    "# Define the Discriminator\n",
    "# ----------------------------\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 1, kernel_size=4, stride=1, padding=0)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sigmoid(self.net(x).view(x.size(0), -1))\n",
    "\n",
    "# ----------------------------\n",
    "# Initialize Models\n",
    "# ----------------------------\n",
    "vqgan = VQGAN().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "optimizer_vq = optim.Adam(vqgan.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_d = optim.Adam(discriminator.parameters(), lr=0.0001, betas=(0.5, 0.999))\n",
    "recon_loss_fn = nn.L1Loss()\n",
    "adv_loss_fn = nn.BCELoss()\n",
    "perceptual_weight = 0.05\n",
    "\n",
    "\n",
    "# Create CSV file and write headers\n",
    "with open(\"training_losses.csv\", mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Epoch\", \"Recon Loss\", \"Adv Loss\", \"D Loss\", \"Perceptual Loss\"])\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop with Image Saving\n",
    "# ----------------------------\n",
    "    for epoch in range(1000):\n",
    "        adv_weight = min(0.1 + epoch * 0.0005, 0.5)  # Gradually increase adversarial weight\n",
    "    \n",
    "        for images, _ in dataloader:\n",
    "            images = images.to(device)\n",
    "\n",
    "        # Train Discriminator\n",
    "            optimizer_d.zero_grad()\n",
    "            real_preds = discriminator(images)\n",
    "            fake_images, _ = vqgan(images)\n",
    "            fake_preds = discriminator(fake_images.detach())\n",
    "            real_loss = adv_loss_fn(real_preds, torch.ones_like(real_preds))\n",
    "            fake_loss = adv_loss_fn(fake_preds, torch.zeros_like(fake_preds))\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "            d_loss.backward()\n",
    "            optimizer_d.step()\n",
    "\n",
    "        # Train VQGAN (Generator)\n",
    "            optimizer_vq.zero_grad()\n",
    "            fake_preds = discriminator(fake_images)\n",
    "            adv_loss = adv_loss_fn(fake_preds, torch.ones_like(fake_preds))\n",
    "            recon_loss = recon_loss_fn(fake_images, images)\n",
    "            perceptual_loss_val = perceptual_loss(fake_images, images)\n",
    "            total_loss = recon_loss + adv_weight * adv_loss + perceptual_weight * perceptual_loss_val\n",
    "            total_loss.backward()\n",
    "            optimizer_vq.step()\n",
    "\n",
    "    # Save losses at the end of each epoch\n",
    "        writer.writerow([epoch + 1, recon_loss.item(), adv_loss.item(), d_loss.item(), perceptual_loss_val.item()])\n",
    "    # Save images every 100 epochs & final epoch\n",
    "        if epoch % 100 == 0 or epoch == 999:\n",
    "            with torch.no_grad():\n",
    "                sample_images, _ = next(iter(dataloader))\n",
    "                sample_images = sample_images.to(device)\n",
    "                recon_sample, _ = vqgan(sample_images)\n",
    "                vutils.save_image(recon_sample, f\"for report_epoch_{epoch}.png\", normalize=True)\n",
    "    \n",
    "        print(f\"Epoch {epoch+1}/1000 | Recon Loss: {recon_loss.item():.4f} | Adv Loss: {adv_loss.item():.4f} | D Loss: {d_loss.item():.4f} | Perceptual Loss: {perceptual_loss_val.item():.4f}\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(vqgan.state_dict(), \"report2novgg_vqgan.pth\")\n",
    "torch.save(discriminator.state_dict(), \"report2nongg_discriminator.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc59df-3078-47fc-a061-462b57ce812d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
